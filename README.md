# NLP-Text-Generation

This Code demonstrates how to generate text using a word-based RNN. I worked with a dataset of Shakespeare's Plays Using Effectiveness of Recurrent Neural Networks. Given a sequence of words from this data ("Shakespear"), train a model to predict the next few wordss in the sequence . Longer sequences of text can be generated by calling the model repeatedly.



While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:

The model is Word-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.

The structure of the output resembles a playâ€”blocks of text generally , similar to the dataset.

As demonstrated below, the model is trained on small batches of text , and is still able to generate a longer sequence of text with coherent structure.
